"""
ç€è¦½å™¨å·¥å…·æ¨¡çµ„
æä¾›ç¶²é ç€è¦½å’Œå…§å®¹æå–åŠŸèƒ½
"""

import re
import time

from strands import tool

from config.prompts import get_browser_prompt, get_error_message
from config.settings import settings
from utils.logger import get_logger

logger = get_logger(__name__)

# ç€è¦½å™¨æœå‹™åˆå§‹åŒ–ç‹€æ…‹
browser_service = None
browser_available = False


def init_browser_service():
    """åˆå§‹åŒ–ç€è¦½å™¨æœå‹™"""
    global browser_service, browser_available

    if not settings.BROWSER_ENABLED:
        logger.info("ç€è¦½å™¨åŠŸèƒ½å·²åœç”¨")
        return False

    try:
        from services.browser_service import BrowserService

        browser_service = BrowserService(settings.AWS_REGION)
        browser_available = browser_service.is_available()
        logger.info(f"ğŸŒ ç€è¦½å™¨æœå‹™åˆå§‹åŒ–: {'æˆåŠŸ' if browser_available else 'å¤±æ•—'}")
        return browser_available
    except Exception as e:
        logger.error(f"ç€è¦½å™¨æœå‹™åˆå§‹åŒ–éŒ¯èª¤: {str(e)}", exc_info=True)
        browser_available = False
        return False


@tool
def browse_website_official(task_description: str) -> str:
    """
    ä½¿ç”¨å®˜æ–¹ Playwright + AgentCore Browser æ•´åˆç€è¦½ç¶²ç«™

    Args:
        task_description: ç€è¦½ä»»å‹™æè¿°ï¼Œä¾‹å¦‚ï¼š
                        - "è¨ªå• https://example.com ä¸¦æå–é é¢æ¨™é¡Œ"
                        - "ç€è¦½ https://news.com ä¸¦ç¸½çµä¸»è¦å…§å®¹"
                        - "æŸ¥çœ‹ https://docs.aws.amazon.com çš„æ–‡ä»¶"

    Returns:
        str: ç€è¦½çµæœçš„è©³ç´°æè¿°
    """
    logger.info(f"ğŸŒ ä½¿ç”¨å®˜æ–¹ Playwright ç€è¦½å™¨é–‹å§‹ä»»å‹™: {task_description[:100]}...")

    try:
        from bedrock_agentcore.tools.browser_client import browser_session
        from playwright.sync_api import sync_playwright

        # ä½¿ç”¨å®˜æ–¹çš„ browser_session å’Œ Playwright æ•´åˆ
        with browser_session(settings.AWS_REGION) as client:
            logger.info("âœ… AgentCore Browser æœƒè©±å·²å»ºç«‹")

            # ç²å– WebSocket URL å’Œ headers
            ws_url, headers = client.generate_ws_headers()
            logger.info("âœ… WebSocket é€£æ¥è³‡è¨Šå·²ç”Ÿæˆ")

            with sync_playwright() as playwright:
                # é€é CDP é€£æ¥åˆ°é ç«¯ Chrome ç€è¦½å™¨
                browser = playwright.chromium.connect_over_cdp(ws_url, headers=headers)
                context = browser.contexts[0]
                page = context.pages[0]

                try:
                    # è§£æä»»å‹™æè¿°ï¼Œå˜—è©¦æå– URL
                    urls = extract_urls(task_description)

                    if urls:
                        # å¦‚æœæ‰¾åˆ° URLï¼Œè¨ªå•ç¬¬ä¸€å€‹
                        target_url = urls[0]
                        logger.info(f"ğŸ¯ è¨ªå•ç›®æ¨™ URL: {target_url}")

                        # æª¢æŸ¥æ˜¯å¦ç‚º PDF æª”æ¡ˆ
                        if target_url.lower().endswith(".pdf"):
                            logger.warning(get_browser_prompt("pdf_warning"))

                        # å°èˆªåˆ°ç›®æ¨™é é¢
                        page.goto(
                            target_url, wait_until="networkidle", timeout=settings.BROWSER_TIMEOUT
                        )

                        # ç²å–é é¢æ¨™é¡Œ
                        title = page.title()
                        logger.info(f"ğŸ“„ é é¢æ¨™é¡Œ: {title}")

                        # å˜—è©¦æå–é é¢å…§å®¹
                        content = extract_page_content(page)

                        # æ§‹å»ºçµæœ
                        result = format_browse_result(target_url, title, content)

                    else:
                        # æ²’æœ‰æ‰¾åˆ° URLï¼Œå›å‚³èªªæ˜
                        result = get_error_message("invalid_url")

                    # ç­‰å¾…ä¸€å°æ®µæ™‚é–“ç¢ºä¿é é¢å®Œå…¨è¼‰å…¥
                    time.sleep(2)

                finally:
                    # ç¢ºä¿æ¸…ç†è³‡æº
                    try:
                        page.close()
                        browser.close()
                        logger.info("âœ… ç€è¦½å™¨è³‡æºå·²æ¸…ç†")
                    except Exception as cleanup_error:
                        logger.warning(f"âš ï¸ ç€è¦½å™¨è³‡æºæ¸…ç†è­¦å‘Š: {cleanup_error}")

                return result

    except Exception as e:
        error_msg = f"å®˜æ–¹ç€è¦½å™¨å·¥å…·åŸ·è¡Œå¤±æ•—: {str(e)}"
        logger.error(f"âŒ {error_msg}", exc_info=True)
        return get_error_message("browser_navigation_failed", error=str(e))


@tool
def browse_website_backup(task_description: str) -> str:
    """
    å‚™ç”¨ç€è¦½å™¨å·¥å…· - ä½¿ç”¨æ­£ç¢ºçš„ AgentCoreBrowser èª¿ç”¨æ ¼å¼
    ç•¶å®˜æ–¹ Playwright æ–¹æ³•å¤±æ•—æ™‚ä½¿ç”¨æ­¤å‚™ç”¨æ–¹æ¡ˆ

    Args:
        task_description: ç€è¦½ä»»å‹™æè¿°ï¼Œéœ€åŒ…å« URL

    Returns:
        str: ç€è¦½çµæœæè¿°
    """
    # ç¢ºä¿ç€è¦½å™¨æœå‹™å·²åˆå§‹åŒ–
    if not browser_available and not init_browser_service():
        return "âŒ å‚™ç”¨ç€è¦½å™¨åŠŸèƒ½ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ç€è¦½å™¨å·¥å…·åˆå§‹åŒ–ç‹€æ…‹ã€‚"

    logger.info(f"ğŸ”„ ä½¿ç”¨å‚™ç”¨ç€è¦½å™¨å·¥å…·: {task_description[:100]}...")

    # æå– URL
    urls = extract_urls(task_description)

    if not urls:
        return get_error_message("invalid_url")

    target_url = urls[0]

    try:
        result = browser_service.browse_with_backup(target_url, task_description)
        logger.info("âœ… å‚™ç”¨ç€è¦½å™¨ä»»å‹™å®Œæˆ")
        return result

    except Exception as e:
        error_msg = f"å‚™ç”¨ç€è¦½å™¨å·¥å…·åŸ·è¡ŒéŒ¯èª¤: {str(e)}"
        logger.error(f"âŒ {error_msg}", exc_info=True)
        return get_error_message("browser_navigation_failed", error=str(e))


def extract_urls(text: str) -> list:
    """
    å¾æ–‡å­—ä¸­æå– URL

    Args:
        text: åŒ…å« URL çš„æ–‡å­—

    Returns:
        URL åˆ—è¡¨
    """
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    return re.findall(url_pattern, text)


def extract_page_content(page) -> str:
    """
    æå–é é¢å…§å®¹

    Args:
        page: Playwright page ç‰©ä»¶

    Returns:
        æå–çš„å…§å®¹æ–‡å­—
    """
    try:
        logger.info(get_browser_prompt("extracting_content"))

        # ç²å–ä¸»è¦æ–‡å­—å…§å®¹
        content = page.evaluate("""
            () => {
                // ç§»é™¤è…³æœ¬å’Œæ¨£å¼æ¨™ç±¤
                const scripts = document.querySelectorAll('script, style, nav, footer, aside');
                scripts.forEach(el => el.remove());

                // å˜—è©¦ç²å–ä¸»è¦å…§å®¹
                let mainContent = document.querySelector('main, article, .content, .main-content, #main, #content');
                if (!mainContent) {
                    mainContent = document.body;
                }

                return mainContent ? mainContent.innerText.trim() : document.body.innerText.trim();
            }
        """)

        # é™åˆ¶å…§å®¹é•·åº¦é¿å…éé•·
        if content and len(content) > 2000:
            content = content[:2000] + "\n\n" + get_browser_prompt("content_truncated")

        return content if content else get_error_message("content_extraction_failed")

    except Exception as e:
        logger.warning(f"âš ï¸ å…§å®¹æå–å¤±æ•—: {e}")
        return get_error_message("content_extraction_failed")


def format_browse_result(url: str, title: str, content: str) -> str:
    """
    æ ¼å¼åŒ–ç€è¦½çµæœ

    Args:
        url: ç¶²å€
        title: é é¢æ¨™é¡Œ
        content: é é¢å…§å®¹

    Returns:
        æ ¼å¼åŒ–çš„çµæœå­—ä¸²
    """
    result = "ğŸŒ ç¶²ç«™ç€è¦½çµæœï¼š\n\n"
    result += f"ğŸ“„ æ¨™é¡Œ: {title}\n"
    result += f"ğŸ”— ç¶²å€: {url}\n\n"
    result += f"ğŸ“ å…§å®¹æ‘˜è¦:\n{content}\n"

    return result


# åˆå§‹åŒ–ç€è¦½å™¨æœå‹™ï¼ˆå»¶é²è¼‰å…¥ï¼‰
# é€™å°‡åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ™‚åˆå§‹åŒ–
