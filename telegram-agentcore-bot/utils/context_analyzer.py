"""
Context å¤§å°åˆ†æžå™¨
å¹«åŠ©è¨ºæ–· token é™åˆ¶å’Œ context éŽå¤§å•é¡Œ
"""

from typing import Any

from utils.logger import get_logger

logger = get_logger(__name__)

# Token ä¼°ç®—ï¼šå¹³å‡æ¯å€‹å­—å…ƒç´„ 0.25 tokensï¼ˆä¸­æ–‡ï¼‰æˆ– 0.33 tokensï¼ˆè‹±æ–‡ï¼‰
# ä¿å®ˆä¼°è¨ˆä½¿ç”¨ 0.4
CHARS_PER_TOKEN = 2.5


def estimate_tokens(text: str | list | dict) -> int:
    """
    ä¼°ç®—æ–‡å­—çš„ token æ•¸é‡

    Args:
        text: æ–‡å­—ã€åˆ—è¡¨æˆ–å­—å…¸

    Returns:
        ä¼°ç®—çš„ token æ•¸é‡
    """
    if isinstance(text, (list, dict)) or not isinstance(text, str):
        text = str(text)

    char_count = len(text)
    estimated_tokens = int(char_count / CHARS_PER_TOKEN)

    return estimated_tokens


def analyze_context_size(
    messages: Any = None,
    memory_context: Any = None,
    tool_results: Any = None,
    images: list | None = None,
) -> dict[str, Any]:
    """
    åˆ†æž context å„éƒ¨åˆ†çš„å¤§å°

    Args:
        messages: æ¶ˆæ¯å…§å®¹
        memory_context: Memory æª¢ç´¢çš„å…§å®¹
        tool_results: å·¥å…·åŸ·è¡Œçµæžœ
        images: åœ–ç‰‡åˆ—è¡¨

    Returns:
        åˆ†æžçµæžœå­—å…¸
    """
    analysis = {
        "messages_chars": 0,
        "messages_tokens": 0,
        "memory_chars": 0,
        "memory_tokens": 0,
        "tool_results_chars": 0,
        "tool_results_tokens": 0,
        "images_count": 0,
        "images_tokens": 0,  # æ¯å¼µåœ–ç‰‡ç´„ ~1000 tokens
        "total_tokens": 0,
        "is_large": False,
        "warning_level": "normal",  # normal, warning, critical
    }

    # åˆ†æžæ¶ˆæ¯
    if messages:
        messages_str = str(messages)
        analysis["messages_chars"] = len(messages_str)
        analysis["messages_tokens"] = estimate_tokens(messages_str)

    # åˆ†æž Memory context
    if memory_context:
        memory_str = str(memory_context)
        analysis["memory_chars"] = len(memory_str)
        analysis["memory_tokens"] = estimate_tokens(memory_str)

    # åˆ†æžå·¥å…·çµæžœ
    if tool_results:
        tool_str = str(tool_results)
        analysis["tool_results_chars"] = len(tool_str)
        analysis["tool_results_tokens"] = estimate_tokens(tool_str)

    # åˆ†æžåœ–ç‰‡
    if images:
        analysis["images_count"] = len(images)
        analysis["images_tokens"] = len(images) * 1000  # æ¯å¼µåœ–ç‰‡ç´„ 1000 tokens

    # è¨ˆç®—ç¸½ tokens
    analysis["total_tokens"] = (
        analysis["messages_tokens"]
        + analysis["memory_tokens"]
        + analysis["tool_results_tokens"]
        + analysis["images_tokens"]
    )

    # åˆ¤æ–·å¤§å°ç´šåˆ¥
    if analysis["total_tokens"] > 150000:  # >150K tokens
        analysis["is_large"] = True
        analysis["warning_level"] = "critical"
    elif analysis["total_tokens"] > 100000:  # >100K tokens
        analysis["is_large"] = True
        analysis["warning_level"] = "warning"

    return analysis


def log_context_analysis(
    analysis: dict[str, Any], user_id: str | None = None, operation: str = "process_message"
) -> None:
    """
    è¨˜éŒ„ context åˆ†æžçµæžœåˆ°æ—¥èªŒ

    Args:
        analysis: åˆ†æžçµæžœ
        user_id: ç”¨æˆ¶ ID
        operation: æ“ä½œé¡žåž‹
    """
    log_extra = {
        "user_id": user_id,
        "operation": operation,
        "total_tokens": analysis["total_tokens"],
        "messages_tokens": analysis["messages_tokens"],
        "memory_tokens": analysis["memory_tokens"],
        "tool_results_tokens": analysis["tool_results_tokens"],
        "images_count": analysis["images_count"],
        "warning_level": analysis["warning_level"],
    }

    if analysis["warning_level"] == "critical":
        logger.warning(
            f"ðŸš¨ Context size CRITICAL: {analysis['total_tokens']} tokens (>150K)",
            extra=log_extra,
        )
    elif analysis["warning_level"] == "warning":
        logger.warning(
            f"âš ï¸ Context size WARNING: {analysis['total_tokens']} tokens (>100K)",
            extra=log_extra,
        )
    else:
        logger.info(f"ðŸ“Š Context size: {analysis['total_tokens']} tokens", extra=log_extra)


def should_truncate_context(analysis: dict[str, Any]) -> bool:
    """
    åˆ¤æ–·æ˜¯å¦æ‡‰è©²æˆªæ–· context

    Args:
        analysis: åˆ†æžçµæžœ

    Returns:
        æ˜¯å¦æ‡‰è©²æˆªæ–·
    """
    # è¶…éŽ 150K tokens å»ºè­°æˆªæ–·
    return analysis["total_tokens"] > 150000


def get_truncation_suggestion(analysis: dict[str, Any]) -> dict[str, str]:
    """
    æä¾› context æˆªæ–·å»ºè­°

    Args:
        analysis: åˆ†æžçµæžœ

    Returns:
        æˆªæ–·å»ºè­°
    """
    suggestions = []

    # Memory ä½”ç”¨éŽå¤§
    if analysis.get("memory_tokens", 0) > 50000:
        suggestions.append("limit_memory")

    # å·¥å…·çµæžœéŽå¤§
    if analysis.get("tool_results_tokens", 0) > 30000:
        suggestions.append("summarize_tool_results")

    # æ¶ˆæ¯éŽé•·
    if analysis.get("messages_tokens", 0) > 50000:
        suggestions.append("truncate_messages")

    return {
        "should_truncate": should_truncate_context(analysis),
        "suggestions": suggestions,
        "reason": f"Total tokens: {analysis.get('total_tokens', 0)} (limit: ~200K)",
    }
